\documentclass[12pt]{article}

% \usepackage[ngerman]{babel}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{csquotes}
\usepackage{ulem}
% \usepackage{hyperref} % use if clickable references wished

% pdfLaTeX font configurations
\usepackage[T1]{fontenc}
\usepackage[mono,vvarbb,upint]{notomath}

% % for CJK characters, compile with XeLaTeX
% \usepackage{fontspec}
% \usepackage[vvarbb,upint]{notomath}
% \setmonofont{Noto Sans Mono}
% \usepackage{xeCJK}
% \setCJKmainfont{Noto Serif CJK TC}

\usepackage[height=9in,width=7in,
    top=68pt,headheight=28pt,headsep=20pt,
    heightrounded]{geometry}
\linespread{1.25}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}

\fancyhead[L]{Hanxin Xia $\vert$ 3417418 \\ WiSe 2023/24}
\fancyhead[R]{Computational Modelling \\ Research Proposal for BN \& AP (M.A.)}
\fancyfoot[C]{\thepage}

% customize section title font
\usepackage{titlesec}
\titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{}

% citation settings
\usepackage{natbib}
\makeatletter
\DeclareRobustCommand\citep
{\begingroup\NAT@swatrue\let\NAT@ctype\z@\NAT@partrue
    \@ifstar{\NAT@fulltrue\NAT@citetp}{\NAT@fullfalse\NAT@citetp}}
\makeatother

\usepackage{qtree}
\usepackage{forest}
\forestset{qtree/.style={baseline, for tree={parent anchor=south,
           child anchor=north,align=center,inner sep=0pt}}}
\useforestlibrary{linguistics}
\forestapplylibrarydefaults{linguistics}

\usepackage{gb4e}
\setlength{\glossglue}{15pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


\noindent \textbf{Proposal 1} --- spaCy performance on named entity recognition task with code-mixed data \bigbreak

The goal of the research is to evaluate how well spaCy performs on NER tasks when it comes to multilingual data, above all where two languages are used interchangeably in one sentence.  The English-Spanish code-mixed data used here comes directly from CALCS 2018 (Computational Approaches to Linguistic Code-Switching) shared task \citep{aguilaretal2018calcs}. Since spaCy out of the box is language specific, the transformer variants of the both involved languages will be used to tag the data. The results (two versions) will be compared to the gold labels given in the CoNLL-U file to retrieve accuracy and possibly other evaluation metrics.

\textbf{Possible expansion for AP} One interesting type of code-switching is the so-called ``insertional code-switching'', where a single token or a short phrase of the embedded language L2 appears inside a sentence structure of the matrix language L1. One could imagine that at least in some cases a language model trained only on L1 would falsely identify inserted L2 elements as special names. My question is, whether there is a correlation between the length of the inserted words and the possibility of them being regarded as named entities. For this the results from above will be re-analysed. The cases of L2 insertions which are not named entities themselves will be extracted with the corresponding word lengths and the annotations returned by spaCy (namely NE == True/False). The percentage of falsely annotated insertions on each word length can then be calculated. Using a correlation test, whether there is a correlation between inserted L2 word length and error rate can then be answered easily.
\vspace{20pt}

\noindent \textbf{Proposal 2} --- NLP model performance on sentiment analysis tasks with code-mixed data \bigbreak

The dataset collected for this proposal is Dataset for Sentiment Analysis on Code-Mix Telugu-English Text \citep{kusampudi2021cssentiment}. The goals and procedures of this BN base research will be largely similar to the previous one except for two critical points: 1) Instead of NER results, the sentiment pipeline provided by \texttt{SpacyTextBlob} will be called up. The results are encoded as \texttt{polarity}; 2) spaCy does not support Telugu. So only the English model can be used. Fundamentally this is a text classification task but with tricky data. Two aspects will be evaluated: 1) How well spaCy English model performs on sentences with English as L1, Telugu as L2; 2) How well the same model performs on sentences with Telugu as L1, English as L2. The results from the latter condition are expected to be much worse than the first one. Because the model itself is not trained on the matrix language, it should not be surprising if the sentiment information of the ``foreign language'' can not be captured effectively.

\textbf{Possible expansion for AP} The unpredictability of the classification results using spaCy lies in that spaCy models are trained solely on monolingual data. A possible way to run a ``real'' monolingual analysis would be integrating multilingual word vectors in the classifier training process. The retrieving of this kind of vectors is doable using language-agnostic models like those from \citet{smith2017multilang}, \citet{devlin2018mbert} and more recently \citet{conneau2019roberta}. My design for this project is to compare the performances of each embedding model on the list on sentiment analysis to determine how well they can capture the semantics of multilingual sentences. The procedure goes as follows: First the data will be split into train and test set. The embeddings of all sentences in both sets will be retrieved using different models. Train different classification models with train embeddings using scikit-learn. Get prediction accuracies on test set embeddings. The results will be organized in tabular form to provide an overview. It will be of great interest to see whether there is a consistent increase of accuracy with embeddings generated by newer models. spaCy's performance could also be brought into comparison as the baseline. One aspect worth further considering is whether a subset of spaCy predictions should be used as the baseline. Since its monolingual nature, including predictions on data with Telugu as matrix language will certainly bring unfair disadvantages to its results. One approach would be by reducing the data to English as L1 only and let spaCy English model compete against other models trained on multilingual embeddings of the same subset of sentences.







\vspace{20pt}

{\setstretch{1.00}
    \bibliographystyle{chicago}
    \bibliography{references}}

\end{document}
