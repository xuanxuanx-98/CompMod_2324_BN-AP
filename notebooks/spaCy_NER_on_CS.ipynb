{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Performance on Named Entity Recognition with Code-Mixed Data\n",
    "\n",
    "In this notebook we examine the multi-functional model spaCy's performances on named entity recognition (NER) tasks, when the data are multilingual. More specifically, we will be focusing on code-mixing (code-switched) data, where the vocabulary of two different languages are used interchangeably in one sentence.\n",
    "\n",
    "#### Data Source\n",
    "\n",
    "The data comes from Computational Approaches to Linguistic Code-Switching (CALCS), which are openly accessible through [LinCE Benchmark](https://ritual.uh.edu/lince/datasets). The specific subset used in this research is the train set in Spanish - English (SPA - ENG) from CALCS Shared Task 2018. The details of the structures of the data will be provides in the first section.\n",
    "\n",
    "#### Sentence Annotation\n",
    "\n",
    "Since spaCy models are usually built on monolingual data, the choice of which language specific model should be used to annotate the current sentence needs to be made based on individual cases. The general pipline goes as follows: \n",
    "\n",
    "1) Determine the matrix language (L1) of the sentence;\n",
    "2) Choose the spaCy model for L1 to annotate the whole sentence, in which tokens of another language (L2) are possibly inserted;\n",
    "3) Retrieve named entity recognition results from [Linguistic Features](https://spacy.io/usage/linguistic-features) built in spaCy standard pipeline;\n",
    "\n",
    "#### Performances Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file\n",
    "file_path = \"../data/train.conll\"\n",
    "\n",
    "# empty list to store DataFrames for each sentence\n",
    "corpus = []\n",
    "\n",
    "# read the CoNLL-U file line by line\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    current_sentence = []\n",
    "    columns = [\"word\", \"lang\", \"entity_type\"]\n",
    "    for line in lines:\n",
    "        if line.startswith(\"# sent_enum\"):\n",
    "            # if a new sentence begins, process the current one\n",
    "            if current_sentence:\n",
    "                df = pd.DataFrame(current_sentence, columns=columns)\n",
    "                corpus.append(df)\n",
    "                current_sentence = []\n",
    "        else:\n",
    "            # append each line to the current sentence\n",
    "            current_sentence.append(line.strip().split(\"\\t\"))\n",
    "\n",
    "# last sentence in the file\n",
    "if current_sentence:\n",
    "    df = pd.DataFrame(current_sentence, columns=columns)\n",
    "    corpus.append(df)\n",
    "# each sentence cann now be called by corpus[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spaCy model for both L1 and L2\n",
    "model_eng = spacy.load(\"en_core_web_sm\")\n",
    "model_spa = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_eng_sent(model_eng, corpus, sent_idx):\n",
    "    \"\"\"tag a sentence with English as L1\n",
    "    return a dictionary with language tags, gold NE tags and spacy NER results\"\"\"\n",
    "    sent_df = corpus[sent_idx][:-1]  # remove last row resulted by CoNLL-U seperator\n",
    "\n",
    "    # extract all pre-processed tokens to a list\n",
    "    gold_tokens = list(sent_df[\"word\"])\n",
    "    # regularize gold NER tags, save to list\n",
    "    gold_tags = [\"Yes\" if tag != \"O\" else \"O\" for tag in list(sent_df[\"entity_type\"])]\n",
    "    # also save language tags\n",
    "    gold_langs = list(sent_df[\"lang\"])\n",
    "\n",
    "    sentence_text = sent_df[\"word\"].str.cat(sep=\" \")\n",
    "    doc = model_eng(sentence_text)\n",
    "    nes = [i.text for i in doc.ents]\n",
    "    # flat the nes tokens\n",
    "    nes_tokens = [\n",
    "        item for sublist in [item.split() for item in nes] for item in sublist\n",
    "    ]\n",
    "\n",
    "    if len(nes_tokens) == 0:  # check if spaCy found any NE\n",
    "        spacy_tags = [\"O\"] * len(sent_df)\n",
    "    else:\n",
    "        spacy_tags = []  # list to store spaCy NER results\n",
    "        for token in gold_tokens:\n",
    "            if len(nes_tokens) != 0:\n",
    "                if token in nes_tokens[0] or nes_tokens[0] in token:\n",
    "                    spacy_tags.append(\"Yes\")\n",
    "                    nes_tokens = nes_tokens[1:]\n",
    "                else:\n",
    "                    spacy_tags.append(\"O\")\n",
    "            else:\n",
    "                spacy_tags.append(\"O\")\n",
    "\n",
    "    results = {\n",
    "        \"mlang\": \"eng\",\n",
    "        \"lang\": gold_langs,\n",
    "        \"true_ne\": gold_tags,\n",
    "        \"spacy_ne\": spacy_tags\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_spa_sent(model_spa, corpus, sent_idx):\n",
    "    \"\"\"tag a sentence with English as L1\n",
    "    return a dictionary with language tags, gold NE tags and spacy NER results\"\"\"\n",
    "    sent_df = corpus[sent_idx][:-1]  # remove last row resulted by CoNLL-U seperator\n",
    "\n",
    "    # extract all pre-processed tokens to a list\n",
    "    gold_tokens = list(sent_df[\"word\"])\n",
    "    # regularize gold NER tags, save to list\n",
    "    gold_tags = [\"Yes\" if tag != \"O\" else \"O\" for tag in list(sent_df[\"entity_type\"])]\n",
    "    # also save language tags\n",
    "    gold_langs = list(sent_df[\"lang\"])\n",
    "\n",
    "    sentence_text = sent_df[\"word\"].str.cat(sep=\" \")\n",
    "    doc = model_spa(sentence_text)\n",
    "    nes = [i.text for i in doc.ents]\n",
    "    # flat the nes tokens\n",
    "    nes_tokens = [\n",
    "        item for sublist in [item.split() for item in nes] for item in sublist\n",
    "    ]\n",
    "\n",
    "    if len(nes_tokens) == 0:  # check if spaCy found any NE\n",
    "        spacy_tags = [\"O\"] * len(sent_df)\n",
    "    else:\n",
    "        spacy_tags = []  # list to store spaCy NER results\n",
    "        for token in gold_tokens:\n",
    "            if len(nes_tokens) != 0:\n",
    "                if token in nes_tokens[0] or nes_tokens[0] in token:\n",
    "                    spacy_tags.append(\"Yes\")\n",
    "                    nes_tokens = nes_tokens[1:]\n",
    "                else:\n",
    "                    spacy_tags.append(\"O\")\n",
    "            else:\n",
    "                spacy_tags.append(\"O\")\n",
    "\n",
    "    results = {\n",
    "        \"mlang\": \"spa\",\n",
    "        \"lang\": gold_langs,\n",
    "        \"true_ne\": gold_tags,\n",
    "        \"spacy_ne\": spacy_tags\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/33611 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 33611/33611 [03:05<00:00, 181.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def most_frequent_element(lst):\n",
    "    most_frequent = max(set(lst), key=lst.count)\n",
    "\n",
    "    return most_frequent\n",
    "\n",
    "\n",
    "ner_results = []\n",
    "for i in tqdm(range(len(corpus)), desc=\"Processing\"):\n",
    "    # find the dominant language (lang1=eng, lang2=spa)\n",
    "    lang_tags = list(corpus[i][\"lang\"])\n",
    "    mlang = most_frequent_element(lang_tags)\n",
    "    if mlang == \"lang1\":\n",
    "        ner_results.append(tag_eng_sent(model_eng=model_eng, corpus=corpus, sent_idx=i))\n",
    "    else:\n",
    "        ner_results.append(tag_spa_sent(model_spa=model_spa, corpus=corpus, sent_idx=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis 1\n",
    "\n",
    "How many inserted normal non-NE L2 words are falsely tagged as named entities? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word_idxs: list of indices of inserted L2 tokens that are not NEs in each sentence\n",
    "target_word_idxs = []\n",
    "for result in ner_results:\n",
    "    if result[\"mlang\"] == \"eng\":\n",
    "        # get CS Spanish token index\n",
    "        cs_idx = [i for i in range(len(result[\"lang\"])) if result[\"lang\"][i] == \"lang2\"]\n",
    "        # remove CS Spanish tokens that are actually NEs\n",
    "        cs_ne_idx = [idx for idx in cs_idx if result[\"true_ne\"][idx] == \"O\"]\n",
    "        target_word_idxs.append(cs_ne_idx)\n",
    "    elif result[\"mlang\"] == \"spa\":\n",
    "        # get CS English token index\n",
    "        cs_idx = [i for i in range(len(result[\"lang\"])) if result[\"lang\"][i] == \"lang1\"]\n",
    "        # remove CS English tokens that are actually NEs\n",
    "        cs_ne_idx = [idx for idx in cs_idx if result[\"true_ne\"][idx] == \"O\"]\n",
    "        target_word_idxs.append(cs_ne_idx)\n",
    "\n",
    "cs_fauxne = []  # [(CS tokens count, CS tokens tagged as NE count) of sent_1, ...]\n",
    "# get from spaCy falsely tagged inserted L2 tokens\n",
    "for i in range(len(target_word_idxs)):\n",
    "    if len(target_word_idxs[i]) > 0:\n",
    "        cs_count = len(target_word_idxs[i])\n",
    "        cs_as_ne_count = len(\n",
    "            [j for j in target_word_idxs[i] if ner_results[i][\"spacy_ne\"][j] != \"O\"]\n",
    "        )\n",
    "        cs_fauxne.append((cs_count, cs_as_ne_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4294302626711062"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cs_count = sum(t[0] for t in cs_fauxne)\n",
    "all_cs_as_ne_count = sum(t[1] for t in cs_fauxne)\n",
    "\n",
    "all_cs_as_ne_count / all_cs_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis 2\n",
    "\n",
    "How many falsely tagged tokens are actually normal inserted non-NE L2 words?\n",
    "\n",
    "Namely: How many error are caused by code-switching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word_idxs: list of indices of falsely tagged tokens by spaCy\n",
    "target_word_idxs = []\n",
    "for result in ner_results:\n",
    "    spacy_wrong_ne_idx = [\n",
    "        i\n",
    "        for i, (elem1, elem2) in enumerate(zip(result[\"spacy_ne\"], result[\"true_ne\"]))\n",
    "        if elem1 != elem2\n",
    "    ]\n",
    "    target_word_idxs.append(spacy_wrong_ne_idx)\n",
    "\n",
    "fauxne_at_cs = []  # [(falsely tagged NE count, error on CS position count) of sent_1, ...]\n",
    "for i in range(len(target_word_idxs)):\n",
    "    if len(target_word_idxs[i]) > 0:\n",
    "        fauxne_count = len(target_word_idxs[i])\n",
    "        sentence = ner_results[i]\n",
    "\n",
    "        if sentence[\"mlang\"] == \"eng\":\n",
    "            fauxne_at_cs_count = len([j for j in target_word_idxs[i] if sentence[\"lang\"][j] == \"lang2\"])\n",
    "        elif sentence[\"mlang\"] == \"spa\":\n",
    "            fauxne_at_cs_count = len([j for j in target_word_idxs[i] if sentence[\"lang\"][j] == \"lang1\"])\n",
    "        fauxne_at_cs.append((fauxne_count, fauxne_at_cs_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13780409502746666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fauxne_count = sum(t[0] for t in fauxne_at_cs)\n",
    "all_fauxne_at_cs_count = sum(t[1] for t in fauxne_at_cs)\n",
    "\n",
    "all_fauxne_at_cs_count / all_fauxne_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis 3\n",
    "\n",
    "How many inserted L2 tokens that are actually NEs are correctly identified as NE by L1 model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_word_idxs: list of indices of inserted L2 words that are NEs by gold standard\n",
    "target_word_idxs = []\n",
    "for result in ner_results:\n",
    "    if result[\"mlang\"] == \"eng\":\n",
    "        # get CS Spanish token index\n",
    "        cs_idx = [i for i in range(len(result[\"lang\"])) if result[\"lang\"][i] == \"lang2\"]\n",
    "        # keep CS Spanish tokens that are actually NEs\n",
    "        cs_ne_idx = [idx for idx in cs_idx if result[\"true_ne\"][idx] != \"O\"]\n",
    "        target_word_idxs.append(cs_ne_idx)\n",
    "    elif result[\"mlang\"] == \"spa\":\n",
    "        # get CS English token index\n",
    "        cs_idx = [i for i in range(len(result[\"lang\"])) if result[\"lang\"][i] == \"lang1\"]\n",
    "        # keep CS English tokens that are actually NEs\n",
    "        cs_ne_idx = [idx for idx in cs_idx if result[\"true_ne\"][idx] != \"O\"]\n",
    "        target_word_idxs.append(cs_ne_idx)\n",
    "\n",
    "csne_as_ne = []  # [(L2 tokens = NE count, NE-L2 tokens as NE count) of sent_1, ...]\n",
    "for i in range(len(ner_results)):\n",
    "    if len(target_word_idxs[i]) > 0:\n",
    "        l2ne_count = len(target_word_idxs[i])\n",
    "        l2ne_as_ne_count = len([j for j in target_word_idxs[i] if ner_results[i][\"spacy_ne\"][j] == \"Yes\"])\n",
    "\n",
    "        csne_as_ne.append((l2ne_count, l2ne_as_ne_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6963265306122449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_l2ne_count = sum(t[0] for t in csne_as_ne)\n",
    "all_l2ne_as_ne_count = sum(t[1] for t in csne_as_ne)\n",
    "\n",
    "all_l2ne_as_ne_count / all_l2ne_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_eng(model_eng, corpus, sent_idx):\n",
    "    \"\"\"tag a sentence with English as L1\n",
    "    return updated sentence data frame with NER results in a new column\"\"\"\n",
    "    sent_df = corpus[sent_idx][:-1]  # remove last row resulted by CoNLL-U seperator\n",
    "\n",
    "    # extract all pre-processed tokens to a list\n",
    "    gold_tokens = list(sent_df[\"word\"])\n",
    "    # regularize gold NER tags, save to list\n",
    "    gold_tags = [\"Yes\" if tag != \"O\" else \"O\" for tag in list(sent_df[\"entity_type\"])]\n",
    "    # also save language tags\n",
    "    gold_langs = list(sent_df[\"lang\"])\n",
    "\n",
    "    sentence_text = sent_df[\"word\"].str.cat(sep=\" \")\n",
    "    doc = model_eng(sentence_text)\n",
    "    nes = [i.text for i in doc.ents]\n",
    "    # flat the nes tokens\n",
    "    nes_tokens = [\n",
    "        item for sublist in [item.split() for item in nes] for item in sublist\n",
    "    ]\n",
    "\n",
    "    if len(nes_tokens) == 0:  # check if spaCy found any NE\n",
    "        spacy_tags = [\"O\"] * len(sent_df)\n",
    "    else:\n",
    "        spacy_tags = []  # list to store spaCy NER results\n",
    "        for token in gold_tokens:\n",
    "            if len(nes_tokens) != 0:\n",
    "                if token in nes_tokens[0] or nes_tokens[0] in token:\n",
    "                    spacy_tags.append(\"Yes\")\n",
    "                    nes_tokens = nes_tokens[1:]\n",
    "                else:\n",
    "                    spacy_tags.append(\"O\")\n",
    "            else:\n",
    "                spacy_tags.append(\"O\")\n",
    "\n",
    "    results = {\n",
    "        \"lang\": gold_langs,\n",
    "        \"true_ne\": gold_tags,\n",
    "        \"spacy_ne\": spacy_tags\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompMod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
