% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl2023-mod}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{linguex}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{spaCy Performance on Named Entity Recognition with Code-Mixed Data
\bigbreak \normalsize BN Project Abstract for Seminar Computational Modelling (WiSe 2023/24)}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Hanxin Xia $\vert$ 3417418\\
  \href{mailto://hanxin.xia@uni-duesseldorf.de}{hanxin.xia@uni-duesseldorf.de}}

\begin{document}
\maketitle


\section{Introduction}

The term ``named entity'' was first defined in the Sixth Message Understanding Conference (MUC-6) \citep{grishman1996ner}. In the publicly available description, at least three types of language expressions were categorized as such entities: ``unique identifiers'' of entities (organizations, persons, locations), times (dates, times), and quantities (monetary values, percentages). The extraction of such entities in a sentence is however not a trivial job. Outside the most obvious dictionary matching \citep{higashinaka2012nerwiki,shang2018learning}, several other hybrid approaches have been proposed over the years, especially for bio-medical other domain specific terms \citep{rock2012nerbio,lou2020dicthyb}. Nowadays, with pre-trained language models spaCy, StanfordNLP as such, named entity recognition tasks can be easily carried out as a part of standard pipeline during text annotation.

This research aims to investigate the multi-purposed language model spaCy's performance on named entity recognition tasks when it comes to multilingual data, above all where two languages are used interchangeably in one sentence, the so-called insertional code-switching, as follows:

\ex. It's just pretending \textit{tengo una ventana aqu√≠}.\\
  \citep[104]{arias2005esencs}

The sentence starts with an English matrix sentence. The language in the embedded clause however, is switched to Spanish (italicized). We want to know how difficult it is for a monolingual spaCy model to extract named entities from such sentences.


\section{Research Questions}





















\newpage
\section{Extracts from BN Proposal}

\noindent \textbf{Proposal 1} --- spaCy performance on named entity recognition task with code-mixed data \bigbreak

The goal of the research is to evaluate how well spaCy performs on NER tasks when it comes to multilingual data, above all where two languages are used interchangeably in one sentence.  The English-Spanish code-mixed data used here comes directly from CALCS 2018 (Computational Approaches to Linguistic Code-Switching) shared task \citep{aguilaretal2018calcs}. Since spaCy out of the box is language specific, the transformer variants of the both involved languages will be used to tag the data. The results (two versions) will be compared to the gold labels given in the CoNLL-U file to retrieve accuracy and possibly other evaluation metrics.

\textbf{Possible expansion for AP} One interesting type of code-switching is the so-called ``insertional code-switching'', where a single token or a short phrase of the embedded language L2 appears inside a sentence structure of the matrix language L1. One could imagine that at least in some cases a language model trained only on L1 would falsely identify inserted L2 elements as special names. My question is, whether there is a correlation between the length of the inserted words and the possibility of them being regarded as named entities. For this the results from above will be re-analysed. The cases of L2 insertions which are not named entities themselves will be extracted with the corresponding word lengths and the annotations returned by spaCy (namely NE == True/False). The percentage of falsely annotated insertions on each word length can then be calculated. Using a correlation test, whether there is a correlation between inserted L2 word length and error rate can then be answered easily.
\vspace{20pt}

\noindent \textbf{Proposal 2} --- NLP model performance on sentiment analysis tasks with code-mixed data \bigbreak

The dataset collected for this proposal is Dataset for Sentiment Analysis on Code-Mix Telugu-English Text \citep{kusampudi2021cssentiment}. The goals and procedures of this BN base research will be largely similar to the previous one except for two critical points: 1) Instead of NER results, the sentiment pipeline provided by \texttt{SpacyTextBlob} will be called up. The results are encoded as \texttt{polarity}; 2) spaCy does not support Telugu. So only the English model can be used. Fundamentally this is a text classification task but with tricky data. Two aspects will be evaluated: 1) How well spaCy English model performs on sentences with English as L1, Telugu as L2; 2) How well the same model performs on sentences with Telugu as L1, English as L2. The results from the latter condition are expected to be much worse than the first one. Because the model itself is not trained on the matrix language, it should not be surprising if the sentiment information of the ``foreign language'' can not be captured effectively.

\textbf{Possible expansion for AP} The unpredictability of the classification results using spaCy lies in that spaCy models are trained solely on monolingual data. A possible way to run a ``real'' monolingual analysis would be integrating multilingual word vectors in the classifier training process. The retrieving of this kind of vectors is doable using language-agnostic models like those from \citet{smith2017multilang}, \citet{devlin2018mbert} and more recently \citet{conneau2019roberta}. My design for this project is to compare the performances of each embedding model on the list on sentiment analysis to determine how well they can capture the semantics of multilingual sentences. The procedure goes as follows: First the data will be split into train and test set. The embeddings of all sentences in both sets will be retrieved using different models. Train different classification models with train embeddings using scikit-learn. Get prediction accuracies on test set embeddings. The results will be organized in tabular form to provide an overview. It will be of great interest to see whether there is a consistent increase of accuracy with embeddings generated by newer models. spaCy's performance could also be brought into comparison as the baseline. One aspect worth further considering is whether a subset of spaCy predictions should be used as the baseline. Since its monolingual nature, including predictions on data with Telugu as matrix language will certainly bring unfair disadvantages to its results. One approach would be by reducing the data to English as L1 only and let spaCy English model compete against other models trained on multilingual embeddings of the same subset of sentences.




% Entries for the entire Anthology, followed by custom entries
\bibliography{references}
\bibliographystyle{acl_natbib}

% \appendix
%
% \section{Example Appendix}
% \label{sec:appendix}

\end{document}
