@inproceedings{aguilaretal2018calcs,
   title = {{Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task}},
   author = "Aguilar, Gustavo and
   AlGhamdi, Fahad and
   Soto, Victor and
   Diab, Mona and
   Hirschberg, Julia and
   Solorio, Thamar",
   booktitle = {{Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching}},
   month = jul,
   year = "2018",
   address = "Melbourne, Australia",
   publisher = "Association for Computational Linguistics",
   url = "https://www.aclweb.org/anthology/W18-3219",
   pages = "138--147"
}

@inproceedings{kusampudi2021cssentiment,
   title = "Sentiment Analysis in Code-Mixed {T}elugu-{E}nglish Text with Unsupervised Data Normalization",
   author = "Kusampudi, Siva Subrahamanyam Varma  and
   Sathineni, Preetham  and
   Mamidi, Radhika",
   editor = "Mitkov, Ruslan  and
   Angelova, Galia",
   booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)",
   month = sep,
   year = "2021",
   address = "Held Online",
   publisher = "INCOMA Ltd.",
   url = "https://aclanthology.org/2021.ranlp-1.86",
   pages = "753--760",
   abstract = "In a multilingual society, people communicate in more than one language, leading to Code-Mixed data. Sentimental analysis on Code-Mixed Telugu-English Text (CMTET) poses unique challenges. The unstructured nature of the Code-Mixed Data is due to the informal language, informal transliterations, and spelling errors. In this paper, we introduce an annotated dataset for Sentiment Analysis in CMTET. Also, we report an accuracy of 80.22{\%} on this dataset using novel unsupervised data normalization with a Multilayer Perceptron (MLP) model. This proposed data normalization technique can be extended to any NLP task involving CMTET. Further, we report an increase of 2.53{\%} accuracy due to this data normalization approach in our best model.",
}

@misc{smith2017multilang,
   title={Offline bilingual word vectors, orthogonal transformations and the inverted softmax},
   author={Samuel L. Smith and David H. P. Turban and Steven Hamblin and Nils Y. Hammerla},
   year={2017},
   eprint={1702.03859},
   archivePrefix={arXiv},
   primaryClass={cs.CL}
}

@article{devlin2018mbert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{conneau2019roberta,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  eprinttype = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
